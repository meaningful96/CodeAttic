{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "IQVgwmNGzRPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZvXy1NPzObq",
        "outputId": "8e6559a3-2375-46d9-e301-6500384b02f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric\n",
        "torch_geometric.__version__"
      ],
      "metadata": {
        "id": "e8BLBZ6MzQW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNN STACK Module"
      ],
      "metadata": {
        "id": "jDR57yXIz1bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_scatter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import Union, Tuple, Optional\n",
        "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
        "                                    OptTensor)\n",
        "\n",
        "from torch.nn import Parameter, Linear\n",
        "from torch_sparse import SparseTensor, set_diag\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
        "\n",
        "class GNNStack(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
        "        super(GNNStack, self).__init__()\n",
        "        conv_model = self.build_conv_model(args.model_type)\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
        "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
        "        for l in range(args.num_layers-1):\n",
        "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
        "\n",
        "        # post-message-passing\n",
        "        self.post_mp = nn.Sequential(\n",
        "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout),\n",
        "            nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.dropout = args.dropout\n",
        "        self.num_layers = args.num_layers\n",
        "\n",
        "        self.emb = emb\n",
        "\n",
        "    def build_conv_model(self, model_type):\n",
        "        if model_type == 'GAT':\n",
        "            return GAT\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        x = self.post_mp(x)\n",
        "\n",
        "        if self.emb == True:\n",
        "            return x\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)"
      ],
      "metadata": {
        "id": "9ZsDJm5UzVLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAT\n",
        "Graph Attention Networks (GATs) are primarily used for node classification in graph-structured data. The key component of GAT is the graph attention layer. Each graph attention layer transforms the input feature vector of a node into a higher-level feature vector. The process consists of the following steps:\n",
        "\n",
        "1. **Linear Transformation**:\n",
        "   Apply a weight matrix $ \\mathbf{W} $ to transform the feature vector of a node.\n",
        "\n",
        "2. **Self-Attention Calculation**:\n",
        "   Compute the attention coefficients that capture the importance of node $i$ to its neighboring nodes $j$.\n",
        "\n",
        "   $$\n",
        "   e_{ij} = a(\\mathbf{W_l}\\overrightarrow{h_i}, \\mathbf{W_r} \\overrightarrow{h_j})\n",
        "   $$\n",
        "   \n",
        "3. **Softmax Normalization**:\n",
        "   Normalize the attention coefficients using a softmax function to obtain $ \\alpha_{ij} $.\n",
        "   $$\n",
        "   \\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}\n",
        "   $$\n",
        "\n",
        "4. **Feature Combination**:\n",
        "   Use the normalized attention coefficients to compute a weighted sum of the neighboring nodes' feature vectors to obtain the final output feature vector.\n",
        "   $$\n",
        "   h_i' = \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{W_r} \\overrightarrow{h_j}\n",
        "   $$\n",
        "\n",
        "5. **Multi-Head Attention**:\n",
        "   Use multiple independent attention mechanisms (heads) to compute the outputs, and then concatenate or average these outputs to obtain the final output feature vector.\n",
        "   $$\n",
        "   \\overrightarrow{h_i}' = ||_{k=1}^K \\Big(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^{(k)} \\mathbf{W_r}^{(k)} \\overrightarrow{h_j}\\Big)\n",
        "   $$\n",
        "\n",
        "Multi-head attention is used to stabilize the learning process and improve performance.\n",
        "\n",
        "-------------------------------------------\n",
        "\n",
        "Graph Attention Networks(GATs)는 그래프 구조의 데이터에 대한 노드 분류를 수행하기 위해 주로 사용된다. GAT의 주요 구성 요소는 그래프 어텐션 레이어이다. 각 그래프 어텐션 레이어는 노드의 입력 특징 벡터를 더 높은 수준의 특징 벡터로 변환한다. 이 과정은 다음과 같은 단계로 구성된다:\n",
        "\n",
        "1. **선형 변환**:\n",
        "   노드의 특징 벡터에 가중치 행렬 $ \\mathbf{W} $를 적용하여 변환한다.\n",
        "\n",
        "2. **셀프 어텐션 계산**:\n",
        "   노드 $i$와 이웃 노드 $j$의 중요도를 계산하는 어텐션 계수를 구한다.\n",
        "\n",
        "   $$\n",
        "   e_{ij} = a(\\mathbf{W_l}\\overrightarrow{h_i}, \\mathbf{W_r} \\overrightarrow{h_j})\n",
        "   $$\n",
        "   \n",
        "3. **소프트맥스 정규화**:\n",
        "   어텐션 계수를 소프트맥스 함수로 정규화하여 $\\alpha_{ij} $를 계산한다.\n",
        "   \n",
        "   $$\n",
        "   \\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}\n",
        "   $$\n",
        "\n",
        "4. **특징 결합**:\n",
        "   정규화된 어텐션 계수를 사용하여 이웃 노드들의 특징 벡터를 가중합하여 최종 출력 특징 벡터를 계산한다.\n",
        "\n",
        "   $$\n",
        "   h_i' = \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{W_r} \\overrightarrow{h_j}\n",
        "   $$\n",
        "\n",
        "5. **멀티 헤드 어텐션**:\n",
        "   여러 개의 독립적인 어텐션 메커니즘(헤드)을 사용하여 각각의 출력을 결합(concatenate)하거나 평균(average)하여 최종 출력 특징 벡터를 얻는다.\n",
        "\n",
        "   $$\n",
        "   \\overrightarrow{h_i}' = ||_{k=1}^K \\Big(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^{(k)} \\mathbf{W_r}^{(k)} \\overrightarrow{h_j}\\Big)\n",
        "   $$\n",
        "\n",
        "멀티 헤드 어텐션은 학습 과정을 안정화하고 성능을 향상시키는 데 사용된다."
      ],
      "metadata": {
        "id": "dk736O5Qzy-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(MessagePassing):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, heads = 2,\n",
        "                 negative_slope = 0.2, dropout = 0., **kwargs):\n",
        "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.negative_slope = negative_slope\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.lin_l = None\n",
        "        self.lin_r = None\n",
        "        self.att_l = None\n",
        "        self.att_r = None\n",
        "\n",
        "        self.lin_l = nn.Linear(self.in_channels, self.out_channels * self.heads)\n",
        "        self.lin_r = self.lin_l\n",
        "\n",
        "        self.att_l = nn.Parameter(torch.zeros(self.heads, self.out_channels))\n",
        "        self.att_r = nn.Parameter(torch.zeros(self.heads, self.out_channels))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin_l.weight)\n",
        "        nn.init.xavier_uniform_(self.lin_r.weight)\n",
        "        nn.init.xavier_uniform_(self.att_l)\n",
        "        nn.init.xavier_uniform_(self.att_r)\n",
        "\n",
        "    def forward(self, x, edge_index, size = None):\n",
        "\n",
        "        H, C = self.heads, self.out_channels\n",
        "        x_l = self.lin_l(x).reshape(-1, H, C)\n",
        "        x_r = self.lin_r(x).reshape(-1, H, C)\n",
        "        alpha_l = self.att_l * x_l\n",
        "        alpha_r = self.att_r * x_r\n",
        "        out = self.propagate(edge_index, x=(x_l, x_r), alpha=(alpha_l, alpha_r), size=size)\n",
        "        out = out.reshape(-1, H*C)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
        "        alpha = F.leaky_relu(alpha_i + alpha_j, negative_slope=self.negative_slope)\n",
        "        if ptr:\n",
        "            att_weight = F.softmax(alpha, ptr)\n",
        "        else:\n",
        "            att_weight = torch_geometric.utils.softmax(alpha, index)\n",
        "        att_weight = F.dropout(att_weight, p=self.dropout)\n",
        "        out = att_weight * x_j\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def aggregate(self, inputs, index, dim_size = None):\n",
        "        out = torch_scatter.scatter(inputs, index, self.node_dim, dim_size=dim_size, reduce='sum')\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "isodoBXCzgHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer"
      ],
      "metadata": {
        "id": "JvqwL2g60bWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "def build_optimizer(args, params):\n",
        "    weight_decay = args.weight_decay\n",
        "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
        "    if args.opt == 'adam':\n",
        "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'sgd':\n",
        "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
        "    elif args.opt == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'adagrad':\n",
        "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    if args.opt_scheduler == 'none':\n",
        "        return None, optimizer\n",
        "    elif args.opt_scheduler == 'step':\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
        "    elif args.opt_scheduler == 'cos':\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
        "    return scheduler, optimizer"
      ],
      "metadata": {
        "id": "uRx0oMSl0YcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def train(dataset, args):\n",
        "\n",
        "    print(\"Node task. test set size:\", np.sum(dataset[0]['train_mask'].numpy()))\n",
        "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "    # build model\n",
        "    model = GNNStack(dataset.num_node_features, args.hidden_dim, dataset.num_classes,\n",
        "                            args)\n",
        "    scheduler, opt = build_optimizer(args, model.parameters())\n",
        "\n",
        "    # train\n",
        "    losses = []\n",
        "    test_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in loader:\n",
        "            opt.zero_grad()\n",
        "            pred = model(batch)\n",
        "            label = batch.y\n",
        "            pred = pred[batch.train_mask]\n",
        "            label = label[batch.train_mask]\n",
        "            loss = model.loss(pred, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "        total_loss /= len(loader.dataset)\n",
        "        losses.append(total_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          test_acc = test(test_loader, model)\n",
        "          test_accs.append(test_acc)\n",
        "          print(\"Epoch \", epoch, \"Loss: \", total_loss, \"Test Acc.: \", test_acc)\n",
        "        else:\n",
        "          test_accs.append(test_accs[-1])\n",
        "    return test_accs, losses\n",
        "\n",
        "def test(loader, model, is_validation=True):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            # max(dim=1) returns values, indices tuple; only need indices\n",
        "            pred = model(data).max(dim=1)[1]\n",
        "            label = data.y\n",
        "\n",
        "        mask = data.val_mask if is_validation else data.test_mask\n",
        "        # node classification: only evaluate on nodes in test set\n",
        "        pred = pred[mask]\n",
        "        label = data.y[mask]\n",
        "\n",
        "        correct += pred.eq(label).sum().item()\n",
        "\n",
        "    total = 0\n",
        "    for data in loader.dataset:\n",
        "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
        "    return correct / total\n",
        "\n",
        "class objectview(object):\n",
        "    def __init__(self, d):\n",
        "        self.__dict__ = d\n"
      ],
      "metadata": {
        "id": "GemlZFR-0eIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GAT\n",
        "def main():\n",
        "    for args in [\n",
        "        {'model_type': 'GAT', 'dataset': 'cora', 'num_layers': 2, 'heads': 1,\n",
        "         'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 500,\n",
        "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,\n",
        "         'weight_decay': 5e-3, 'lr': 0.01},\n",
        "    ]:\n",
        "        args = objectview(args)\n",
        "        for model in ['GAT']:\n",
        "            args.model_type = model\n",
        "\n",
        "            # Match the dimension.\n",
        "            if model == 'GAT':\n",
        "              args.heads = 2\n",
        "            else:\n",
        "              args.heads = 1\n",
        "\n",
        "            if args.dataset == 'cora':\n",
        "                dataset = Planetoid(root='/tmp/cora', name='Cora')\n",
        "            else:\n",
        "                raise NotImplementedError(\"Unknown dataset\")\n",
        "            test_accs, losses = train(dataset, args)\n",
        "\n",
        "            print(\"Maximum accuracy: {0}\".format(max(test_accs)))\n",
        "            print(\"Minimum loss: {0}\".format(min(losses)))\n",
        "\n",
        "            plt.title(dataset.name)\n",
        "            plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
        "            plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "1f0gJqQW0hDM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}